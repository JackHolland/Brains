\documentclass[prl,amsmath,amssymb,floatfix,superscriptaddress,notitlepage,twocolumn]{revtex4}
\usepackage{algorithm2e}
\usepackage{graphicx, tikz}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{xifthen}
\usepackage{color}
\usepackage{datetime}
\usepackage{float}
\usepackage{bm}
\usepackage{changepage}
\usepackage{bbold}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}



%editing commands for the users in our group (You can make more)
\newcommand{\Damian}[1]{\textcolor{magenta}{(Damian: #1)}}
\newcommand{\Alec}[1]{\textcolor{blue}{(Alec: #1)}}
\newcommand{\Jack}[1]{\textcolor{green}{(Jack: #1)}}
\newcommand{\Rawan}[1]{\textcolor{red}{(Rawan: #1)}}


%commands to make math expressions easier
\newcommand{\ee}[1]{\begin{align} #1 \end{align}} 						%align environment
\newcommand{\vc}[1]{\vec{\mathbf{#1}}} 								%arrowed bold vector
\newcommand{\intg}[2]{\int \! \mathrm{d}^{#1}\vc{#2} \ }					%integral with measure
\newcommand{\nn}[1][]{\ifthenelse{\isempty{#1}}{\nonumber \\}{\nonumber}}	%nonumber shortcut
\newcommand{\dv}{\partial }											%partial derivative shortcut
\newcount\colveccount												%column vector
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{bmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{bmatrix}
        \fi
}



\begin{document}

\title{Percieving Circadian Gene Expression through Artificial Neural Networks}

\author{Damian Sowinski}
\email{Damian.Sowinski.GR@dartmouth.edu}
\affiliation{Center for Cosmic Origins \\Department of Physics and Astronomy, Dartmouth College}

\author{Alexander Crowell}
\email{Alexander.M.Crowell.GR@dartmouth.edu}
\affiliation{Department of Genetics, Geisel School of Medicine, Dartmouth College}

\author{Jack Holland}
\email{Jack.E.Holland.GR@dartmouth.edu}
\affiliation{Department of Computer Science, Dartmouth College,Hanover, NH 03755, USA}

\author{Rawan Al Ghofaili}
\email{Rawan.al.Ghofaili.GR@dartmouth.edu}
\affiliation{Department of Computer Science, Dartmouth College,Hanover, NH 03755, USA}


\date{Last Updated: \today, \currenttime}

\begin{abstract}

Science is done!

\end{abstract}

\maketitle

\section{Introduction}
According to scientist, \cite{paper1}, blah blah blah

\section{Circadian Rhythms}
Circadian Clocks are an adaptive response to the evolutionary necessity of anticipating day night cycles.  They occur widely in higher eukaryotes and sparsely in fungi and cyanobacteria.  Because of the widespread and longstanding evolutionary history of organisms specializing their activity to a specific phase of the day night cycle, circadian clocks, where present, have been found to regulate almost every biological process imaginable.  Notable among these are the cell cycle, DNA damage repair and metabolism, processes central to the majority of human disease \cite{Dunlap99}.  Perturbation of the clock upsets these processes as well and has been shown to contribute to disease \cite{Bass10}.  One of the most important recent trends in circadian research has been the interaction of the circadian cycle with disease treatment, specifically the timed delivery of medicine \cite{Zhang14}.  Additionally the circadian clock's modulation of such a wide variety of gene and protein expression along with the diverse processes employed to achieve that modulation provides an excellent opportunity for the study of gene regulation at every stage from transcription to degradation \cite{Mehra09,Menet14}.

The current state of the art in the identification of Circadian gene expression is the JTK\_CYCLE algorithm (JTK) \cite{Hughes10}.  JTK uses a nonparametric test for monotonic orderings in the data over all possible period lengths and phases and for high (\~2hr) resolution data it provides high sensitivity, specifity and computational efficiency.  JTK and earlier approaches including curve fitting \cite{Straume04}, autocorrelation \cite{Levine02}, Fourier analysis \cite{Wichert04,Whitfield04} and Fisher's G Test \cite{Wichert04} all fall short however as the resolution of data decreases.  Due to technical challenges in the maintenance and synchronization of cells as well as sequencing costs, dataset resolution is currently a major limiting factor in the identification of circadian genes.  Therefore an improved computational technique for identifying circadian genes in low temporal resolution data could rapidly provide knew knowledge from publicly available pre-existing datasets

\section{Artificial Neural Networks}
In this section we go over the theoretical background of how to build an ANN, develop the formalism behind the backpropogation algorithm, and discuss possible candidates for tunable hyperparameters/functions used. We will develop both the feed forward and back prop algorithms for completeness. 

ANNs are a supervised machine learning algorithm used for classification problems. Given some labeled set of data $\mathcal{D}=\{(\vc x,\vc y)_i\}_{i=1}^D$, where to each multidimensional datum $\vc x$ there is an associated multidimensional label $\vc y$, the ANN learns from $\mathcal{D}$ how to place labels on new data. Processing a datum by the network is accomplished via the \textit{feed forward} algorithm, while training is done through the \textit{back propogation} algorithm. 

The basic building block of an ANN is the perceptron. $N$ signals go into the perceptron, are weighted, summed, and then put through some activation function to create an output signal:
\ee{
\label{perceptron}
\text{output}=\theta(\sum_{i=0}^{N}w_i\times\text{input}_i)
}
The form of the \textbf{activation function}, $\theta$ is typcally chosen to be one that gives a value of $1$ for arguments larger than some threshhold, and $0$ otherwise. The backpropogation algorithm requires this function to be both differentiable and invertible, so typical choices include the logistic, arctan, and hyperbolic tangent functions. Note also that the sum begins at $0$, leading to $N+1$ inputs. The $0^{th}$ input is always taken to be $1$, and its corresponding weight is known as the \textbf{bias}.

We can stack perceptrons into layers, and in turn stack those together so that the inputs of the $l^{th}$ layer are the outputs coming from the $(l-1)^{th}$ layer. This is the basic form of an ANN. We need only specify the architecture of the ANN (number of layers/\{Input dimensionality,number of perceptrons in each layer\}), which will be $L/\{N_0,N_1,\dots,N_L\}$. Note that $N_0$ represents the dimensionality of the input signal, and $N_L$ is the dimensionality of the output layer. Between each pair of layers there is a corresponsing weight matrix, $\textbf{w}^{(l)}:\mathbb{R}^{N_{l-1}}\rightarrow\mathbb{R}^{N_l}$, the components of which are $w_{ij}^{(l)}$, where $l$ is the target layer, $i$ is the index of the \textit{to} perceptron in the target layer, and $j$ is the \textit{from} perceptron in the domain layer. We denote the set of all these matrices \textbf{W}, and refer to this set as the \textbf{architecture} of our ANN. With these definitions in hand, we can write the output of the $i^{th}$ perceptron in the $l^{th}$ layer in compact form:
\ee{
x^{(l)}_i=\theta(\sum_{j=0}^{N_{l-1}}w^{(l)}_{ij}x^{(l-1)}_j)
}
where once again,, the $w^{(l)}_{i0}$'s represent biases. 

With the architecture in place, we can now easily state the feed forward algorithm, \ref{feed_forward}. It is nothing more than allowing an input signal propagate through the network, and eventually pop out at the end.

This is all fine and dandy, but what we want is to be able to train the network based on the known labels. We need to compare the output of the network with the known label, define an error based on that comparison, and then alter the weights in the network so as to decrease that error. Furthermore, when the weights are changed, we do not want them to take on any values (one can imagine one weight becoming so high that it dominates the flow of signals in the ANN), so we must also regularize the way in which the error function is causing the weights to change. These statements can be made more precise.

<<<<<<< Updated upstream
Initialize an ANN with randomized weights. Take a datum $(\vc x^{(0)},\vc y)$ and have the ANN process the input to get an output $(\vc x^{(L)},\vc y)$. We define the cost of the function as an explicit function of the output, the \textbf{error}, and an explicit function of the weights, the \textbf{regularizer}:
\ee{\label{CostFactorization}
C(\vc x^{(L)},\vc y,\textbf{W})=E(\vc x^{(L)},\vc y)+R(\textbf{W})
}
This factorization of the arguments is a bit deceptive, since the output is an implicit function of the weights of the network. This fact will be crucial as we start exploring weight-space and will need to take gradients of the cost with respect to the weights. Our goal will be to decrease the cost of our ANN at each iteration by changing the weights of the network. Over the course of many epochs, the network will learn the patterns in the data, and we will have minimized the cost. So, how do we choose the change in weights to achieve this at each iteration?

We want:
\ee{
C(\vc x^{(L)},\vc y,\textbf{W}+\delta\textbf{W})\le C(\vc x^{(L)},\vc y,\textbf{W})\nn[1]
}
Expanding the RHS to first order in $\delta\textbf{W}$ we have:
\ee{
\nabla C\cdot \delta\textbf{W}\le0\nn[1]
}
where the gradient is taken with respect to the weights. We can easilly satisfy this equation if we choose:
\ee{
\label{LearningRule}
\delta\textbf{W}&=-\eta \nabla C\nn
\Downarrow&\nn
\delta w^{(l)}_{ij}&=-\eta\frac{dC}{d w^{(l)}_{ij}}
}
This is our \textbf{learning rule}. The hyperparameter $\eta$ is called the \textbf{learning rate}. Taking the gradient of the regularizer is simple, since it is an explicit function of the weights. To take the gradient of the error, however, we will have to employ some mathematical trickery. We have to use the fact that the output depends on the weights, making the error an implicit function of the weights. 

First off we will break up the reweighing of the ANN layer by layer. So we will start at the output layer, and move backward. To proceed we need one result on how to differentiate an output neuron with respect to weight:
\ee{
\frac{d x^{(l)}_n}{d w^{(l)}_{ij}}=&\frac{d}{d w^{(l)}_{ij}} \theta(\sum_{k=0}^{N_{l-1}}w^{(l)}_{nk}x^{(l-1)}_k)\nn
=&\theta'\circ\theta^{-1}(x^{(l)}_n)\delta_{ni}x^{(l-1)}_j
}
if we define $\phi=\theta'\circ\theta^{-1}$, the gradient of the error can then be written:
\ee{
\frac{d E}{d w^{(L)}_{ij}}=&\sum_{k=1}^{N_L}\frac{\dv E}{\dv x_k^{(L)}}\frac{d x^{(l)}_k}{d w^{(l)}_{ij}}\nn
=&\frac{\dv E}{\dv x_i^{(L)}}\phi(x_i^{(L)})x_j^{(L-1)}
}
We can generalize this for the gradient w.r.t. other layer weights by defining:
\ee{
\delta_i^{(l)}=\left\{
\begin{array}{lr}
       \phi(x_i^{(l)})\frac{\dv E}{\dv x_i^{(l)}} & l=L\\
       \phi(x_i^{(l)})\sum_j\delta^{(l+1)}_jw^{(l+1)}_{ji} & l<L
\end{array}
\right.
}
This is known as the $\delta$-rule, and is used to write the learning rate in its compact form:
\ee{
<<<<<<< Updated upstream
\delta w^{(l)}_{ij}=-\eta\left(\frac{dR}{d w^{(l)}_{ij}}+\delta^{(l)}_ix^{(l-1)}_j\right)
=======
\frac{d E}{d w^{(L)}_{ij}}=&\sum_{k=0}^{N_L}
=======
Take a datum $(\vc x^{(0)},\vc y)$ and process the input to get an output $d=(\vc x^{(L)},\vc y)$. Then determine the cost of this processed output and the weights of the network via:
\ee{
C(
>>>>>>> Stashed changes
>>>>>>> Stashed changes
}
This is all we need to define the back propogation algorithm, \ref{back_prop}.


To go any farther we need to discuss the possible choices for the functions that define our network: the activation, error, and regularizer. We will now discuss each of these in turn. 

\subsection{Function Choices}
\subsubsection{Activation Function}
The only constraints that we have on our choice is that the function be both differentiable and invertible. Typical choices are therefore any smooth strictly monotonic function. We should also specify the domain of the inputs. The list below has been normalized to the range $(0,1)$, and we have included the backprop functions for completeness:
\begin{itemize}
\item Sigmoid (blue)
\ee{
\theta(x)=&\frac{1}{1+e^{-x}}\\
\phi(x) =& x(1-x)\nn[1]
}
\item Hyperbolic Tangent (magenta)
\ee{
\theta(x) =& \frac{1}{2}(1+\text{tanh} x)\\
\phi(x) =& 2x(1-x)\nn[1]
}
\item Arctangent (gold)
\ee{
\theta(x) =& \frac{1}{2}(1+\frac{2}{\pi}\text{arctan} x)\\
\phi(x) =& \frac{1}{\pi}\cos^2\frac{\pi}{2}(2x-1)\nn[1]
}
\end{itemize}


The activations are plotted in figure \ref{AF}. The qualitative differences are mostly with the arctan function, which requires incredibly high absolute value inputs to generate output close to $0$ or $1$. The corresponding backprop functions are plotted in figure.

\begin{figure*}[ht!]
\label{AF}
\centering
\includegraphics[width=0.48\textwidth]{ActivationFunctions.png}\hfill
\includegraphics[width=0.48\textwidth]{BackPropActivationFunctions.png}
\caption{On the left we have the activation functions plotted, and the back propagated functions on the right. The sigmoid, hyperbolic tangent, and arctangent are blue, magenta, and gold, respectively. Not that the main difference }
\end{figure*}


\subsection{Algorithms}


\begin{algorithm}[H]
\caption{Feed Forward}\label{feed_forward}
\begin{algorithmic}[]
\Procedure{FeedForward}{$\vc x^{(0)}$,\textbf{W}}\\
\State $L \gets \text{number of layers in }\textbf{W}$
\State $N_l \gets \text{number of perceptrons in } \textbf{w}^{(l)}\in\textbf{W}$
\State $N_0 \gets \text{dim(domain(}(\vc x^{(0)})$\\

\State \textbf{for} $l=1,2,\dots,L$
\State \hspace{.25cm}$x^{(l-1)}_0 \gets 1$
\State \hspace{.25cm}\textbf{for} $i=1,2,\dots,N_l$
\State \hspace{.25cm}\hspace{.25cm}$x^{(l)}_i\gets\theta(\sum_{j=0}^{N_{l-1}}w^{(l)}_{ij}x^{(l-1)}_j)$\Comment{ and store}
\State \hspace{.25cm}\textbf{end}
\State \textbf{end}\\

\State \textbf{return} \textbf{X}$=(\vc x^{(0)},\vc x^{(1)},\dots,\vc x^{(L)})$\\
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Back Propogation}\label{back_prop}
\begin{algorithmic}[]
\Procedure{BackProp}{\textbf{X}, $\vc y$, \textbf{W}}\\
\State $L \gets \text{number of layers in }\textbf{W}$
\State $N_l \gets \text{dimensionality of each } \vc x^{(l)} \in \textbf{X}$\\

\State \textbf{for} $l=L,L-1,\dots,,2,1$
\State \hspace{.25cm}\textbf{for} $i = 1,2,\cdots, N_l$
\State \hspace{.50cm}\textbf{if} $l=L$
\State \hspace{.75cm} $\delta^{(l)}_i \gets \phi(x_i^{(l)})\nabla_i E$
\State \hspace{.50cm}\textbf{else}
\State \hspace{.75cm} $\delta^{(l)}_i\gets \phi(x^{(l)}_i)\sum_{j=0}^{N_{l+1}}\delta^{(l+1)}_jw^{(l+1)}_{ji}$
\State \hspace{.50cm}\textbf{end}
\State \hspace{.50cm}\textbf{for} $j=1,2,\dots,N_{l-1}$
\State \hspace{.75cm}$w^{(l)}_{ij}\gets w^{(l)}_{ij}-\eta\left(\nabla^{(l)}_{ij}R+\delta^{(l)}_ix^{(l-1)}_j\right)$
\State \hspace{.50cm}\textbf{end}
\State \hspace{.25cm}\textbf{end}
\State \textbf{end}\\

\State \textbf{return} \textbf{W}\\
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Train Network}\label{train_ANN}
\begin{algorithmic}[]
\Procedure{Train}{$\mathcal{D}$, \textbf{W}, numEpochs}\\

\State \textbf{for} $i = 1, 2, \dots,$ numEpochs
\State \hspace{.25cm} $\textbf{randomize}(\mathcal{D})$ 
\State \hspace{.25cm} $\textbf{for } d\in\mathcal{D}$
\State \hspace{.50cm} $\textbf{X} \gets \text{FeedForward}(d_1,\textbf{W})$
\State \hspace{.50cm} $\textbf{W} \gets \text{BackProp}(\textbf{X}, d_2,\textbf{W})$
\State \hspace{.25cm} \textbf{end}
\State \textbf{end}\\

\State \textbf{return W}\\

\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Cross Validation}\label{cross_val}
\begin{algorithmic}[]
\Procedure{CrossVal}{$\mathcal{D}$, \textbf{W}, numEpochs, N}\\

\State \textbf{randomize}$(\mathcal{D})$
\State \textbf{partition} $\mathcal{D}=\mathcal{D}^1\sqcup\mathcal{D}^2\sqcup\dots\sqcup\mathcal{D}^N$
\State \textbf{for } $i=1,2,\dots,N$
\State \hspace{.25cm} $\textbf{W}\gets \text{Train}(\mathcal{D\backslash \mathcal{D}}^i, \textbf{W}, \text{numEpochs})$
\State \hspace{.25cm} MSE$_i\gets\|\mathcal{D}^i_2-\text{FeedForward}(\mathcal{D}^i_1,\textbf{W})\|_2$
\State \textbf{end}
\State CVerror $\gets \text{mean}(\{MSE_i\})$\\
\State \textbf{return} CVerror\\

\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Data}

\begin{figure*}[!ht]
  \centering
  \subfloat{\includegraphics[width=.4\textwidth]{Circadian.png}}\quad
  \subfloat{\includegraphics[width=.4\textwidth]{Non-Circadian.png}}\\
  \subfloat{\includegraphics[width=.4\textwidth]{Unclassified.png}}\quad
  \subfloat{\includegraphics[width=.4\textwidth]{PvsQ.pdf}}
  \caption{Hierarchically clustered heatmaps of each class along with a plot of P vs Q for all genes}
  \label{fig:1}
\end{figure*}

The training dataset employed in this project was derived from Neurospora Crassa a filamentous fungus which is one of the primary model organisms in the study of circadian clocks due to it's genetic tractability, ease of culture and possession of a semantically similar clock to higher eukaryotes.  All samples were synchronized with a light to dark transition and three biological replicates were collected every two hours for 48 hours.  These samples were then sequenced and aligned to the neurospora genome using tophat.  Feature counting was performed with HTSeq count and normalization performed with the DESeq bioconductor package.  The resulting gene counts were then processed using JTK for Circadian classification.  The training data is comprised of a combination of the normalized gene counts at each timepoint and a classifier of circadian, non-circadian or unclassified based on the adjusted P value output by JTK. The cutoffs for these classifiers were determined by examining the P and Q value distributions for all genes along with hierarchically clustered heatmaps for each set.




\section{Experiment}

\section{Conclusion}

<<<<<<< Updated upstream
\bibliographystyle{unsrt}
\bibliography{bibliography}

=======
\begin{thebibliography}{99}
\bibitem{paper1} J. Bekenstein, Phys. Rev. D {\bf 23}, 287 (1981).
\end{thebibliography}
>>>>>>> Stashed changes

\end{document}