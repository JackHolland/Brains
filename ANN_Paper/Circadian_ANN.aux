\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Dunlap99}
\citation{Bass10}
\citation{Zhang14}
\citation{Mehra09,Menet14}
\citation{Hughes10}
\citation{Straume04}
\citation{Levine02}
\citation{Wichert04,Whitfield04}
\citation{Wichert04}
\newlabel{FirstPage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Circadian Rhythms}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {}Artificial Neural Networks}{1}{section*.2}}
\newlabel{perceptron}{{1}{1}{}{equation.0.1}{}}
\newlabel{CostFactorization}{{3}{2}{}{equation.0.3}{}}
\newlabel{LearningRule}{{4}{2}{}{equation.0.4}{}}
\citation{Field96}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Function Choices}{3}{section*.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Activation Function}{3}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Error Function}{3}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Regularizer}{3}{section*.6}}
\newlabel{AF10}{{}{4}{}{equation.0.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces On the left we have the activation functions plotted, and the back propagated functions on the right. The sigmoid, hyperbolic tangent, and arctangent are blue, magenta, and gold, respectively. Not that the main difference }}{4}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Equations of motion}{4}{section*.7}}
\newlabel{EoM}{{21}{4}{}{equation.0.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Data}{5}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {}Experiment}{5}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Hierarchically clustered heatmaps of each class along with a plot of P vs Q for all genes. You can see the oscillatory behavior of the circadian genes oover the course of 48hrs. The unclassified genes have some oscillatory behavior, though not on a 24-hour cycle. Non-cicadian data have no discernable oscillatory behavior. The lower right plot displays the cut-offs used to generate the classes. }}{6}{figure.2}}
\newlabel{data_classes}{{2}{6}{Hierarchically clustered heatmaps of each class along with a plot of P vs Q for all genes. You can see the oscillatory behavior of the circadian genes oover the course of 48hrs. The unclassified genes have some oscillatory behavior, though not on a 24-hour cycle. Non-cicadian data have no discernable oscillatory behavior. The lower right plot displays the cut-offs used to generate the classes}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Final Remarks}{6}{section*.10}}
\newlabel{EG1}{{}{7}{}{section*.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The error rates during training for two network architectures on a logarithmic scale. Both the $L_1$ and $L_2$ errors decrease as more iterations occur. They tend to flatline by 100 iterations, however this could be indicative of overfitting the training data. }}{7}{figure.3}}
\newlabel{EG2}{{}{7}{}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The winner take all error, and confidence as measured byt he $L_1$-norm for both the trainging data (red) and the test data (blue). Both begin to plateau after about 40 iterations. }}{7}{figure.4}}
\bibdata{bibliography}
\bibcite{Dunlap99}{{1}{}{{}}{{}}}
\bibcite{Bass10}{{2}{}{{}}{{}}}
\bibcite{Zhang14}{{3}{}{{}}{{}}}
\bibcite{Mehra09}{{4}{}{{}}{{}}}
\bibcite{Menet14}{{5}{}{{}}{{}}}
\bibcite{Hughes10}{{6}{}{{}}{{}}}
\bibcite{Straume04}{{7}{}{{}}{{}}}
\bibcite{Levine02}{{8}{}{{}}{{}}}
\bibcite{Field96}{{9}{}{{}}{{}}}
\global \chardef \firstnote@num9\relax 
\@writefile{toc}{\contentsline {section}{\numberline {}Bibliography}{8}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{8}{section*.12}}
\newlabel{LastBibItem}{{9}{8}{}{section*.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Algorithms}{8}{section*.13}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feed Forward}}{8}{algorithm.1}}
\newlabel{feed_forward}{{1}{8}{}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Back propagation}}{8}{algorithm.2}}
\newlabel{back_prop}{{2}{8}{}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Train Network}}{8}{algorithm.3}}
\newlabel{train_ANN}{{3}{8}{}{algorithm.3}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\bibstyle{unsrt}
\newlabel{LastPage}{{}{9}{}{page.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Cross Validation}}{9}{algorithm.4}}
\newlabel{cross_val}{{4}{9}{}{algorithm.4}{}}
